{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOf3Qb3o91h/m5AWx7jEvzW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import torch\n","import numpy as np\n","from PIL import Image\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from qdrant_client import QdrantClient\n","from qdrant_client.models import PointStruct, VectorParams, Distance\n","import open_clip\n","\n","# Qdrant\n","client = QdrantClient(\n","    url=\"https://7e8950b7-f7cd-476b-9fe2-cfbabcc676d4.us-east4-0.gcp.cloud.qdrant.io:6333\",\n","    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.5-6feBUSirUh0qNrxH8ou2clwuKjY1e_lB_jE4DyUjA\"\n",")\n","\n","# CLIP embedding\n","clip_model_name = \"ViT-B-32\"\n","clip_pretrained = \"openai\"\n","clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(clip_model_name, pretrained=clip_pretrained)\n","clip_tokenizer = open_clip.get_tokenizer(clip_model_name)\n","\n","clip_model.eval()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","clip_model = clip_model.to(device)\n","\n","def embed_query(text):\n","  tokenized = clip_tokenizer([text]).to(device)\n","  with torch.no_grad():\n","      features = clip_model.encode_text(tokenized)\n","  embedding = features[0].cpu().numpy()\n","  return embedding / np.linalg.norm(embedding)\n","\n","from PIL import Image\n","def embed_image(image_file):\n","    image = Image.open(image_file).convert(\"RGB\")\n","    processed = clip_preprocess(image).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        features = clip_model.encode_image(processed)\n","    embedding = features[0].cpu().numpy()\n","    return embedding / np.linalg.norm(embedding)\n","\n","# load llm\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","login(token='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')\n","\n","llama_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n","llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)\n","llama_model = AutoModelForCausalLM.from_pretrained(\n","    llama_model_id,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",").eval()\n","\n","\n","# generate answer function\n","def generate_llama_answer(prompt, max_tokens=512):\n","  inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n","  with torch.no_grad():\n","      outputs = llama_model.generate(\n","          **inputs,\n","          max_new_tokens=max_tokens,\n","          do_sample=True,\n","          temperature=0.7,\n","          top_p=0.9\n","      )\n","  decoded = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","  if \"Answer:\" in decoded:\n","      return decoded.split(\"Answer:\")[-1].strip()\n","  else:\n","      return decoded.strip()\n","\n","\n","# prompt\n","def build_multimodal_prompt_with_image(user_query, hits, image_uploaded=True, max_products=3):\n","    context_blocks = []\n","    for i, h in enumerate(hits[:max_products]):\n","        p = h.payload\n","        name = p.get(\"Product_Name\", \"Unknown Product\")\n","        price = p.get(\"Selling_Price\", \"N/A\")\n","        about = p.get(\"About_Product\", \"No description provided.\")\n","        image_url = p.get(\"Image_URL\", \"No image available.\")\n","\n","        block = f\"\"\"Product {i+1}:\n","Name: {name}\n","Price: ${price}\n","Key Info: {about}\n","Image: {image_url}\"\"\"\n","        context_blocks.append(block)\n","\n","    context = \"\\n\\n\".join(context_blocks)\n","\n","    # Add fallback few-shot example for image queries\n","    fallback_example = \"\"\"Example:\n","User Query: [Image of an iPhone]\n","Context: No Apple or iPhone product appears in the list.\n","\n","Answer:\n","Sorry, I couldn’t identify the product in the uploaded image based on the current context. Please try uploading a clearer image or rephrasing your query.\n","\"\"\"\n","\n","    # Adjust instructions based on image presence\n","    instructions = f\"\"\"You are a real shopping assistant for an e-commerce platform.\n","\n","A customer has asked a question that may involve an uploaded image, a text query, or both. You have relevant product information below. Answer questions in a warm and friendly manner.\n","\n","Instructions:\n","- Your justifications for your product choice should only include informations and reasonings that are most relevant to the user query.\n","- Use only the product details from the context. Do not guess or invent any information. If nothing relevant is found, apologize and acknowledge that you cannot provide an answer.\n","- The user has NOT seen the product list or context. Your answer should be fully self-contained. Do NOT refer to “Product 1”, “Product 2”, ”query”, or “the context”.\n","- If helpful, you should better include product name and image URL in your answer.\n","- Keep word counts of the answer under 100.\n","\"\"\"\n","\n","    prompt = f\"\"\"{instructions}\n","\n","{fallback_example if image_uploaded else \"\"}\n","\n","Context:\n","{context}\n","\n","User Query:\n","{user_query}\n","\n","Answer:\"\"\"\n","\n","    return prompt\n","\n","# Streamlit App\n","st.set_page_config(page_title=\"🏍️ Product Chatbot\")\n","st.title(\"🏍️ Shopping Assistant\")\n","st.markdown(\"Not sure what you’re looking for? Just upload an image or ask a question!\")\n","\n","user_query = st.text_input(\"Ask a product question:\", placeholder=\"e.g., Is this board good for cruising?\")\n","uploaded_image = st.file_uploader(\"Upload a product image (optional):\", type=[\"jpg\", \"jpeg\", \"png\"])\n","\n","if st.button(\"Submit\") and (user_query or uploaded_image):\n","    st.write(\"Generating response...\")\n","\n","    has_text = bool(user_query)\n","    has_image = uploaded_image is not None\n","\n","    if has_text:\n","        text_emb = embed_query(user_query)\n","    else:\n","        text_emb = None\n","\n","    if has_image:\n","        image_emb = embed_image(uploaded_image)\n","    else:\n","        image_emb = None\n","\n","    # Select embedding and collection\n","    if has_text and has_image:\n","        query_vector = np.concatenate([text_emb, image_emb])\n","        collection_name = \"combined_products\"\n","    elif has_image:\n","        query_vector = image_emb\n","        collection_name = \"image_products\"\n","    elif has_text:\n","        query_vector = text_emb\n","        collection_name = \"text_products\"\n","    else:\n","        st.error(\"Please enter a question or upload an image.\")\n","        st.stop()\n","\n","    query_vector /= np.linalg.norm(query_vector)\n","\n","    raw_result = client.query_points(\n","        collection_name=collection_name,\n","        query=query_vector.tolist(),\n","        limit=5,\n","        with_payload=True\n","    )\n","    hits = raw_result.points\n","\n","    prompt = build_multimodal_prompt_with_image(user_query or \"What is this product?\", hits, image_uploaded=has_image)\n","    answer = generate_llama_answer(prompt)\n","\n","    st.markdown(\"### 🧠 Assistant Answer\")\n","    st.write(answer)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcejFGWdcmOv","executionInfo":{"status":"ok","timestamp":1747550555963,"user_tz":300,"elapsed":21,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"8368a0eb-6548-483b-b9f0-e7180a8a9b78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok, conf\n","import subprocess\n","import time\n","\n","# https://dashboard.ngrok.com/get-started/setup/windows\n","# https://dashboard.ngrok.com/agents\n","\n","conf.get_default().auth_token = \"2xFZbzzcjiXzs9PWN5EYc3YJT03_27V2oxMd2qhAYZDwDCned\"\n","process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n","time.sleep(8)\n","public_url = ngrok.connect(8501)\n","print(\"🌐 Your Streamlit app is live at:\", public_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0BKeIUZfGg2","executionInfo":{"status":"ok","timestamp":1747550564518,"user_tz":300,"elapsed":8542,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"c5329a3a-48bc-4afa-fc7f-e9dbb6ed6530"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🌐 Your Streamlit app is live at: NgrokTunnel: \"https://bd53-34-90-132-26.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":["# pip install qdrant_client"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NV_4D4RvoTKY","executionInfo":{"status":"ok","timestamp":1747877712066,"user_tz":300,"elapsed":3331,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"9b36d566-5f96-421e-a851-b36adbac6cc0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting qdrant_client\n","  Downloading qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (1.71.0)\n","Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.28.1)\n","Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (2.0.2)\n","Collecting portalocker<3.0.0,>=2.7.0 (from qdrant_client)\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (5.29.4)\n","Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (2.11.4)\n","Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (2.4.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.16.0)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (4.13.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.0)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n","Downloading qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Installing collected packages: portalocker, qdrant_client\n","Successfully installed portalocker-2.10.1 qdrant_client-1.14.2\n"]}]},{"cell_type":"code","source":["# pip install open-clip-torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"euoJuCR8oWe1","executionInfo":{"status":"ok","timestamp":1747877803941,"user_tz":300,"elapsed":71392,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"ececb25a-009b-4e8a-f6c8-62b608144a1e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open-clip-torch\n","  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\n","Collecting ftfy (from open-clip-torch)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.31.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.0->open-clip-torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.4.26)\n","Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open-clip-torch\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open-clip-torch-2.32.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ONVnjqUGcX8w","executionInfo":{"status":"ok","timestamp":1747878581264,"user_tz":300,"elapsed":5347,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a044ec8f-ead1-4e52-8b19-e858ac8e4d53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Thank you for reaching out! Based on the product details provided, the DB Longboards Phase 38\" Maple Drop Through Longboard may be suitable for cruising, given its cambered platform and drop-through mounting, which can provide a smoother ride and more comfortable pushing. However, without more context or information about your specific needs and preferences, I cannot make a definitive recommendation. Please feel free to provide additional details or ask further questions!\n"]}],"source":["test= False\n","\n","if test == True:\n","  # Previous work\n","  import torch\n","  import numpy as np\n","  from PIL import Image\n","  from transformers import AutoTokenizer, AutoModelForCausalLM\n","  from qdrant_client import QdrantClient\n","  from qdrant_client.models import PointStruct, VectorParams, Distance\n","  import open_clip\n","\n","  # Qdrant\n","  client = QdrantClient(\n","      url=\"https://7e8950b7-f7cd-476b-9fe2-cfbabcc676d4.us-east4-0.gcp.cloud.qdrant.io:6333\",\n","      api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.5-6feBUSirUh0qNrxH8ou2clwuKjY1e_lB_jE4DyUjA\"\n","  )\n","\n","  # CLIP embedding\n","  clip_model_name = \"ViT-B-32\"\n","  clip_pretrained = \"openai\"\n","  clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(clip_model_name, pretrained=clip_pretrained)\n","  clip_tokenizer = open_clip.get_tokenizer(clip_model_name)\n","\n","  clip_model.eval()\n","  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","  clip_model = clip_model.to(device)\n","\n","  def embed_query(text):\n","    tokenized = clip_tokenizer([text]).to(device)\n","    with torch.no_grad():\n","        features = clip_model.encode_text(tokenized)\n","    embedding = features[0].cpu().numpy()\n","    return embedding / np.linalg.norm(embedding)\n","\n","  from PIL import Image\n","  def embed_image(image_file):\n","      image = Image.open(image_file).convert(\"RGB\")\n","      processed = clip_preprocess(image).unsqueeze(0).to(device)\n","      with torch.no_grad():\n","          features = clip_model.encode_image(processed)\n","      embedding = features[0].cpu().numpy()\n","      return embedding / np.linalg.norm(embedding)\n","\n","  # load llm\n","  from transformers import AutoTokenizer, AutoModelForCausalLM\n","  from huggingface_hub import login\n","  login(token='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')\n","\n","  llama_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n","  llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)\n","  llama_model = AutoModelForCausalLM.from_pretrained(\n","      llama_model_id,\n","      torch_dtype=torch.float16,\n","      device_map=\"auto\"\n","  ).eval()\n","\n","\n","  # generate answer function\n","  def generate_llama_answer(prompt, max_tokens=512):\n","    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n","    with torch.no_grad():\n","        outputs = llama_model.generate(\n","            **inputs,\n","            max_new_tokens=max_tokens,\n","            do_sample=True,\n","            temperature=0.7,\n","            top_p=0.9\n","        )\n","    decoded = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    if \"Answer:\" in decoded:\n","        return decoded.split(\"Answer:\")[-1].strip()\n","    else:\n","        return decoded.strip()\n","\n","\n","  # prompt\n","  def build_multimodal_prompt_with_image(user_query, hits, image_uploaded=True, max_products=3):\n","      context_blocks = []\n","      for i, h in enumerate(hits[:max_products]):\n","          p = h.payload\n","          name = p.get(\"Product_Name\", \"Unknown Product\")\n","          price = p.get(\"Selling_Price\", \"N/A\")\n","          about = p.get(\"About_Product\", \"No description provided.\")\n","          image_url = p.get(\"Image_URL\", \"No image available.\")\n","\n","          block = f\"\"\"Product {i+1}:\n","  Name: {name}\n","  Price: ${price}\n","  Key Info: {about}\n","  Image: {image_url}\"\"\"\n","          context_blocks.append(block)\n","\n","      context = \"\\n\\n\".join(context_blocks)\n","\n","      # Add fallback few-shot example for image queries\n","      fallback_example = \"\"\"Example:\n","  User Query: [Image of an iPhone]\n","  Context: No Apple or iPhone product appears in the list.\n","\n","  Answer:\n","  Sorry, I couldn’t identify the product in the uploaded image based on the current context. Please try uploading a clearer image or rephrasing your query.\n","  \"\"\"\n","\n","      # Adjust instructions based on image presence\n","      instructions = f\"\"\"You are a real shopping assistant for an e-commerce platform.\n","\n","  A customer has asked a question that may involve an uploaded image, a text query, or both. You have relevant product information below. Answer questions in a warm and friendly manner.\n","\n","  Instructions:\n","  - Your justifications for your product choice should only include informations and reasonings that are most relevant to the user query.\n","  - Use only the product details from the context. Do not guess or invent any information. If nothing relevant is found, apologize and acknowledge that you cannot provide an answer.\n","  - The user has NOT seen the product list or context. Your answer should be fully self-contained. Do NOT refer to “Product 1”, “Product 2”, ”query”, or “the context”.\n","  - If helpful, you should better include product name and image URL in your answer.\n","  - Keep word counts of the answer under 100.\n","  \"\"\"\n","\n","      prompt = f\"\"\"{instructions}\n","\n","  {fallback_example if image_uploaded else \"\"}\n","\n","  Context:\n","  {context}\n","\n","  User Query:\n","  {user_query}\n","\n","  Answer:\"\"\"\n","\n","      return prompt\n","\n","  # images and query\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  image_path = \"/content/drive/MyDrive/Gen_AI/LongBoards.jpg\"\n","  query_text = \"Is this board good for cruising?\" # OR：I want a lightweight longboard for carving and cruising.\n","\n","  # embedding\n","  with open(image_path, \"rb\") as f:\n","    img_emb = embed_image(f)\n","\n","  txt_emb = embed_query(query_text)\n","\n","  # combine images and text embeddings\n","  combined_emb = np.concatenate([txt_emb, img_emb])\n","  combined_emb /= np.linalg.norm(combined_emb)\n","\n","  # retrieve top context\n","  raw_result = client.query_points(\n","      collection_name=\"combined_products\",\n","      query=combined_emb.tolist(),\n","      limit=5,\n","      with_payload=True\n","  )\n","  hits = raw_result.points\n","\n","  # get prompt\n","  prompt = build_multimodal_prompt_with_image(query_text, hits, image_uploaded=True)\n","\n","  # get answer\n","  answer = generate_llama_answer(prompt)\n","  print(answer)"]},{"cell_type":"code","source":["if test == True:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  image_path = \"/content/drive/MyDrive/Gen_AI/toy.jpg\"\n","  query_text = \"Recommend similar products for me with images\"\n","  with open(image_path, \"rb\") as f:\n","    img_emb = embed_image(f)\n","\n","  txt_emb = embed_query(query_text)\n","  combined_emb = np.concatenate([txt_emb, img_emb])\n","  combined_emb /= np.linalg.norm(combined_emb)\n","  raw_result = client.query_points(\n","      collection_name=\"combined_products\",\n","      query=combined_emb.tolist(),\n","      limit=5,\n","      with_payload=True\n","  )\n","  hits = raw_result.points\n","  prompt = build_multimodal_prompt_with_image(query_text, hits, image_uploaded=True)\n","  answer = generate_llama_answer(prompt)\n","  print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXawgqvjqA5o","executionInfo":{"status":"ok","timestamp":1747880540851,"user_tz":300,"elapsed":5175,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"55f865bd-ab5a-4716-8488-77a44bac4333"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Thank you for reaching out! Based on the product details provided, I'd recommend the Little Live Pets - My Kissing Puppy - Wrinkles. It's a cute and cuddly toy that makes realistic puppy sounds and has different actions, just like a real puppy! The image URL for this product is https://images-na.ssl-images-amazon.com/images/I/414mqi2RZLL.jpg. Would you like me to provide more recommendations or help you with anything else?\n"]}]},{"cell_type":"code","source":["if test == True:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  image_path = \"/content/drive/MyDrive/Gen_AI/ladder.jpg\"\n","  query_text = \"What is the maximum weight capacity of this ladder? How tall is this ladder when fully opened? Does this ladder have anti-slip feet or other safety features?\"\n","  with open(image_path, \"rb\") as f:\n","    img_emb = embed_image(f)\n","\n","  txt_emb = embed_query(query_text)\n","  combined_emb = np.concatenate([txt_emb, img_emb])\n","  combined_emb /= np.linalg.norm(combined_emb)\n","  raw_result = client.query_points(\n","      collection_name=\"combined_products\",\n","      query=combined_emb.tolist(),\n","      limit=5,\n","      with_payload=True\n","  )\n","  hits = raw_result.points\n","  prompt = build_multimodal_prompt_with_image(query_text, hits, image_uploaded=True)\n","  answer = generate_llama_answer(prompt)\n","  print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yhi7zucI3zG0","executionInfo":{"status":"ok","timestamp":1747881853015,"user_tz":300,"elapsed":5894,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"1e7b1629-9cfd-4d5d-90fd-1fc318dc2f4c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Thank you for your question! Based on the product information provided, the Hasegawa Ladders Lucano Step Ladder has a maximum weight capacity of 225 lbs. The ladder's height when fully opened is 7.5 feet. As for anti-slip feet or other safety features, the product description mentions that the ladder is sturdy and safe, holding up to 225 lbs. weight. However, I couldn't find any specific mention of anti-slip feet or other safety features in the product details. Please let me know if you have any further questions!\n"]}]},{"cell_type":"code","source":["if test == True:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  image_path = \"/content/drive/MyDrive/Gen_AI/songbook.jpg\"\n","  query_text = \"What educational skills does this toy help develop? Also recommend me similar products with images\"\n","  with open(image_path, \"rb\") as f:\n","    img_emb = embed_image(f)\n","  txt_emb = embed_query(query_text)\n","  combined_emb = np.concatenate([txt_emb, img_emb])\n","  combined_emb /= np.linalg.norm(combined_emb)\n","  raw_result = client.query_points(collection_name=\"combined_products\",query=combined_emb.tolist(),limit=5,with_payload=True)\n","  hits = raw_result.points\n","  prompt = build_multimodal_prompt_with_image(query_text, hits, image_uploaded=True)\n","  answer = generate_llama_answer(prompt)\n","  print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgkBOqWY5vrS","executionInfo":{"status":"ok","timestamp":1747882344720,"user_tz":300,"elapsed":11821,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"465af1f2-4fac-4676-a483-c02775bb533e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Based on the product details provided, the toy \"3-in-1 Around We Go Activity Center\" helps develop various educational skills, including:\n","\n","1. Motor Skills: As the child moves the toy around the activity center, they develop their hand-eye coordination and fine motor skills.\n","2. Problem-Solving: The toy encourages children to problem-solve and think critically as they navigate around the center.\n","3. Creativity: The toy fosters creativity and imagination as children explore and interact with the different activities and objects.\n","\n","Recommended similar products with images:\n","1. \"Learning Journey Activity Center\" - This product provides a similar interactive experience with different activities and toys for children to explore. Image: https://images-na.ssl-images-amazon.com/images/I/61%2B-fA0ng2L.jpg\n","2. \"Hape Activity Center\" - This product offers a wooden activity center with different toys and activities for children to engage with. Image: https://images-na.ssl-images-amazon.com/images/I/51ZRl8YQW-L.jpg\n","\n","Please note that these recommendations are based on the product details provided and may not be exhaustive or accurate for all products.\n"]}]},{"cell_type":"code","source":["if test == True:\n","  query_text = \"My daughter wants a bedsheet with cartoon characters. Can you recommend some good options with urls?\"\n","  txt_emb = embed_query(query_text)\n","  txt_emb /= np.linalg.norm(txt_emb)\n","  raw_result = client.query_points(collection_name=\"text_products\",  query=txt_emb.tolist(),limit=5,with_payload=True)\n","  hits = raw_result.points\n","  prompt = build_multimodal_prompt_with_image(query_text, hits, image_uploaded=False)\n","  answer = generate_llama_answer(prompt)\n","  print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-vErIti-bh5","executionInfo":{"status":"ok","timestamp":1747884252334,"user_tz":300,"elapsed":15502,"user":{"displayName":"Xinyu Wang","userId":"16848783801859566033"}},"outputId":"b1652a54-c3f6-46a4-a296-e5f792477a00"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Of course, I'd be happy to help! 😊 There are some adorable cartoon character-themed bed sheets available on our platform. Here are a few options that caught my eye:\n","\n","1. Unbranded Emoji, Cool Dude, 3-Piece Comforter Set Twin: This set features a fun and funny emoji-themed design with bright colors and cute illustrations. Your daughter will love the soft brushed fabric and the novelty pattern. Check out the image here: <https://images-na.ssl-images-amazon.com/images/I/51iVCRtMVrL.jpg>\n","2. Sweet Jojo Designs Ballet Dancer Ballerina Queen Kids Children's Bed Skirt: This bed skirt features a beautiful ballet-themed design with colorful illustrations of ballerinas. It's made of 100% cotton and is machine washable. Take a look at the image here: <https://images-na.ssl-images-amazon.com/images/I/311GVPNhFEL.jpg>\n","3. Urban Habitat Kids Lola Full/Queen Duvet Cover Set Girls Bedding - Purple, Aqua, Unicorns: This duvet cover set features a whimsical unicorn-themed design with bright colors and fun patterns. The set includes a duvet cover, two shams, and two decorative pillows. Check out the image here: <https://images-na.ssl-images-amazon.com/images/I/51jMEcCFeuL.jpg>\n","\n","All of these options are available on our platform and come with positive reviews from satisfied customers. I hope this helps you find the perfect bed sheet for your daughter! 😊\n"]}]}]}